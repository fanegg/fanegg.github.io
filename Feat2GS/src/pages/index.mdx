---
layout: ../layouts/Layout.astro
title: "Feat2GS"
subtitle: "Probing Visual Foundation Models with Gaussian Splatting"
description: Project Page for Feat2GS
favicon: cat.svg
thumbnail: screenshot.png
---
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import QuoteSection from "../components/QuoteSection.astro";
import NoteSection from "../components/NoteSection.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import RadarChart from '../components/RadarChart.astro';

import CompareVideos from "../components/CompareVideos.astro";
import MergeVideo from '../components/MergeVideo.astro';
import plushies_rgb from "../assets/Application/plushies_rgb.mp4";
import plushies_depth from "../assets/Application/plushies_depth.mp4";

import pipe from "../assets/pipe.mp4";
import motivation from "../assets/motivation.png";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

import { Metric3dData } from "../data/Metric3d";
import { GeometryData } from "../data/Geometry.js";
import { TextureData, TextureConfig } from "../data/Texture.js";
import { AllData } from "../data/All.js";
import { ApplicationData } from "../data/Application.js";

<Header
  title={frontmatter.title}
  subtitle={frontmatter.subtitle}
  authors={[
    {
      name: "Yue Chen",
      url: "https://fanegg.github.io/",
      notes: ["1"],
    },
    {
      name: "Xingyu Chen",
      url: "https://rover-xingyu.github.io/",
      notes: ["1"],
    },
    {
      name: "Anpei Chen",
      url: "https://apchenstu.github.io/",
      notes: ["1,3"],

    },
    {
      name: "Gerard Pons-Moll",
      url: "https://virtualhumans.mpi-inf.mpg.de/",
      notes: ["3,4"],
    },
    {
      name: "Yuliang Xiu",
      url: "https://xiuyuliang.cn/",
      notes: ["1,2"],
    },
  ]}
  institutions={[
    { symbol: "¹", name: "Westlake University" },
    { symbol: "²", name: "Max Planck Institute for Intelligent Systems" },
    { symbol: "³", name: "University of Tübingen, Tübingen AI Center" },
    { symbol: "⁴", name: "Max Planck Institute for Informatics, Saarland Informatics Campus" },
  ]} 
  links={[
    {
      name: "Paper",
      url: "https://arxiv.org/abs/2412.09606",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code (Coming Soon)",
      url: "https://fanegg.github.io/Feat2GS/",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2412.09606",
      icon: "academicons:arxiv",
    },
  ]}
  />

<Image source={motivation} altText="Motivation" className="w-full max-w-[800px] mx-auto px-4 sm:px-0 overflow-x-hidden" />

<div className="text-center max-w-3xl mx-auto">
  We present **Feat2GS**, a unified framework to probe _"texture and geometry awareness"_ of visual foundation models. Novel view synthesis serves as an effective proxy for 3D evaluation.
</div>

 ## How it works   

  <Video source={pipe} controls={false} />
  Casually captured photos are input into visual foundation models (VFMs) to extract features and into a stereo reconstructor to obtain relative poses. Pixel-wise features are transformed into 3D Gaussians (3DGS) using a lightweight readout layer trained with photometric loss. 3DGS parameters, grouped into **G**eometry and **T**exture, enable separate analysis of geometry/texture awareness in VFMs, evaluated by novel view synthesis (NVS) quality on diverse, unposed open-world images. We conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve SOTA across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for NVS.

## Video
  <YouTubeVideo videoId="4fT5lzcAJqo" />

## <a href="#chart" id="chart" className="no-underline hover:no-underline text-inherit outline-none cursor-default">Average for Novel View Synthesis across six datasets</a>
<RadarChart className="my-8" />

## <a href="#dtu" id="dtu" className="no-underline hover:no-underline text-inherit outline-none cursor-default">Novel View Synthesis _aligns well with_ Pointcloud Error Map</a>
<CompareVideos videos={Metric3dData} className="w-full sm:w-2/3 mx-auto" />

## Geometry Probing
<CompareVideos videos={GeometryData} />

## Texture Probing
<CompareVideos videos={TextureData} defaultLabels={TextureConfig.defaultLabels} />

## All=Geometry+Texture Probing
<CompareVideos videos={AllData} className="w-full sm:w-2/3 mx-auto" />

## Application
<MergeVideo videos={ApplicationData} />


## BibTeX citation

```bibtex
@article{chen2024feat2gs,
  title = {Feat2GS: Probing Visual Foundation Models with Gaussian Splatting},
  author = {Chen, Yue and Chen, Xingyu and Chen, Anpei and Pons-Moll, Gerard and Xiu, Yuliang},
  journal={arXiv preprint arXiv:2412.09606},
  year = {2024},
}
```