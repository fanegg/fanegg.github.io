---
layout: ../layouts/Layout.astro
title: "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting"
subtitle: "Probing Visual Foundation Models with Gaussian Splatting"
description: Project Page for Feat2GS
favicon: cat.svg
thumbnail: screenshot.jpg
---
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import QuoteSection from "../components/QuoteSection.astro";
import NoteSection from "../components/NoteSection.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import RadarChart from '../components/RadarChart.astro';
import Acknowledgement from "../components/Acknowledgement.astro";

import CompareVideos from "../components/CompareVideos.astro";
import MergeVideo from '../components/MergeVideo.astro';

import pipe from "../assets/pipe.mp4";
import motivation from "../assets/motivation.png";
import Gprobe from "../assets/Gprobe.png";
import Tprobe from "../assets/Tprobe.png";
import Aprobe from "../assets/Aprobe.png";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

import { Metric3dData } from "../data/Metric3d";
import { GeometryData } from "../data/Geometry.js";
import { TextureData, TextureConfig } from "../data/Texture.js";
import { AllData } from "../data/All.js";
import { ApplicationData } from "../data/Application.js";

<Header
  title={frontmatter.title}
  subtitle={frontmatter.subtitle}
  authors={[
    {
      name: "Yue Chen",
      url: "https://fanegg.github.io/",
      notes: ["1"],
    },
    {
      name: "Xingyu Chen",
      url: "https://rover-xingyu.github.io/",
      notes: ["1"],
    },
    {
      name: "Anpei Chen",
      url: "https://apchenstu.github.io/",
      notes: ["1,2"],

    },
    {
      name: "Gerard Pons-Moll",
      url: "https://virtualhumans.mpi-inf.mpg.de/",
      notes: ["2,3"],
    },
    {
      name: "Yuliang Xiu",
      url: "https://xiuyuliang.cn/",
      notes: ["1,4"],
    },
  ]}
  institutions={[
    { symbol: "1", name: "Westlake University" },
    { symbol: "2", name: "University of Tübingen, Tübingen AI Center" },
    { symbol: "3", name: "Max Planck Institute for Informatics, Saarland Informatics Campus" },
    { symbol: "4", name: "Max Planck Institute for Intelligent Systems" },
  ]} 
  conference="CVPR 2025, Nashville"
  links={[
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2412.09606",
      icon: "academicons:arxiv",
    },
    {
      name: "Code",
      url: "https://github.com/fanegg/Feat2GS",
      icon: "mdi:github",
    },
    {
      name: "Demo",
      url: "https://huggingface.co/spaces/endless-ai/Feat2GS",
      icon: "simple-icons:huggingface",
    },
    {
      name: "Gallery",
      url: "https://fanegg.github.io/feat2gs_gallery.html",
      icon: "fa-solid:images",
    },
    ]}
  />

<Image source={motivation} altText="Motivation" className="w-full max-w-[800px] mx-auto px-4 sm:px-0 overflow-x-hidden" />

<div className="text-center max-w-3xl mx-auto">
  We present **Feat2GS**, a unified framework to probe "texture and geometry awareness" of visual foundation models. Novel view synthesis serves as an effective proxy for 3D evaluation.
</div>

<HighlightedSection>

 ## How it works   
  <div style={{ textAlign: 'justify' }}>
  Casually captured photos are input into visual foundation models (VFMs) to extract features and into a stereo reconstructor to obtain relative poses. Pixel-wise features are transformed into 3D Gaussians (3DGS) using a lightweight readout layer trained with photometric loss. 3DGS parameters, grouped into **G**eometry and **T**exture, enable separate analysis of geometry/texture awareness in VFMs, evaluated by novel view synthesis (NVS) quality on diverse, unposed open-world images. We conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve SOTA across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for NVS.
  </div>
  <Video source={pipe} controls={false} />


</HighlightedSection>

## Video
  <YouTubeVideo videoId="4fT5lzcAJqo" />

## <a href="#chart" id="chart" className="no-underline hover:no-underline text-inherit outline-none cursor-default">Average for Novel View Synthesis across six datasets</a>
<RadarChart className="my-8" />


<div 
  id="dtu"
  style={{
    backgroundColor: '#fff',
    padding: '20px',
    margin: '50px auto 20px',
    boxShadow: '0 0px 10px #999',
    border: '1px solid #ccc',
    borderRadius: '15px',
    display: 'flex',
    alignItems: 'center',
    position: 'relative',
    flexDirection: 'column',
  }}
>
  <h2 style={{ marginTop: '20px', marginBottom: '20px' }}>Pointcloud Error Map _vs._ Novel View Synthesis</h2>

  <h2 style={{ marginTop: '0px', marginBottom: '30px' }}>
    <a 
      href="#dtu" 
      className="no-underline hover:no-underline text-inherit outline-none cursor-default"
      style={{ textDecoration: 'none', color: 'inherit' }} 
    >
      We found 3D Metric and 2D Metric are well-aligned.
    </a>
  </h2>

  <div style={{ flex: 1, textAlign: 'center' }}>
    <CompareVideos
      videos={Metric3dData}
      className="w-full sm:w-2/3 mx-auto"
    />
  </div>
</div>



<div style={{
  backgroundColor: '#fff', 
  padding: '0px 20px 20px 20px',
  margin: '20px auto',
  boxShadow: '0 0px 10px #999',
  border: '1px solid #ccc',
  borderRadius: '15px'
}}>
  <h2 style={{ marginTop: '20px', marginBottom: '30px' }}>Geometry Probing</h2>
  <div className="flex justify-center mb-0">
    <img 
      src={Gprobe.src} 
      alt="Geometry Probing" 
      className="w-1/2 px-4 sm:px-0" 
    />
  </div>
  <div style={{ width: '90%', margin: '0 auto' }}>
    <div style={{ textAlign: 'center', marginBottom: '20px' }}>
      <p style={{ fontSize: '0.95rem', color: '#444', display: 'inline-block', marginTop: '5px' }}>
      When probing geometry awareness, a 2-layer MLP reads out **geometry parameters** from 2D image features,
      while **texture parameters** are freely optimized.
      </p>
    </div>
  </div>
  <CompareVideos videos={GeometryData} />
</div>


<div style={{
  backgroundColor: '#fff', 
  padding: '0px 20px 20px 20px',
  margin: '20px auto',
  boxShadow: '0 0px 10px #999',
  border: '1px solid #ccc',
  borderRadius: '15px'
}}>
  <h2 style={{ marginTop: '20px', marginBottom: '30px' }}>Texture Probing</h2>
  <div className="flex justify-center mb-0">
    <img 
      src={Tprobe.src} 
      alt="Texture Probing" 
      className="w-1/2 px-4 sm:px-0" 
    />
  </div>
  <div style={{ width: '90%', margin: '0 auto' }}>
    <div style={{ textAlign: 'center', marginBottom: '20px' }}>
      <p style={{ fontSize: '0.95rem', color: '#444', display: 'inline-block', marginTop: '5px' }}>
      When probing texture awareness, a 2-layer MLP reads out **texture parameters** from 2D image features,
      while **geometry parameters** are freely optimized.
      </p>
    </div>
  </div>
  <CompareVideos videos={TextureData} defaultLabels={TextureConfig.defaultLabels} />
</div>


<div style={{
  backgroundColor: '#fff', 
  padding: '0px 20px 20px 20px',
  margin: '20px auto',
  boxShadow: '0 0px 10px #999',
  border: '1px solid #ccc',
  borderRadius: '15px'
}}>
  <h2 style={{ marginTop: '20px', marginBottom: '30px' }}>All=Geometry+Texture Probing</h2>
  <div className="flex justify-center mb-0">
    <img 
      src={Aprobe.src} 
      alt="All Probing" 
      className="w-1/2 px-4 sm:px-0" 
    />
  </div>
  <div style={{ width: '90%', margin: '0 auto' }}>
    <div style={{ textAlign: 'center', marginBottom: '20px' }}>
      <p style={{ fontSize: '0.95rem', color: '#444', display: 'inline-block', marginTop: '5px' }}>
      When probing both geometry and texture, a 2-layer MLP simultaneously reads out **all Gaussian parameters**  from 2D image features.
      </p>
    </div>
  </div>
  <CompareVideos videos={AllData} className="w-full sm:w-2/3 mx-auto" />
</div>


## Application
<MergeVideo videos={ApplicationData} />

<h2 className="left-align">Acknowledgement</h2>
<Acknowledgement>
  We thank _**Yuxuan Xue**_, _**Vladimir Guzov**_, _**Garvita Tiwari**_ for their valuable feedback, and the members of _**Endless AI Lab**_ and _**Real Virtual Humans**_ for their help and discussions. This work is funded by the Research Center for Industries of the Future (RCIF) at Westlake University, the Westlake Education Foundation, the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 409792180 (EmmyNoether Programme, project: Real Virtual Humans), and German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A.  _**Yuliang Xiu**_ also received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No.860768 (CLIPE project). _**Gerard Pons-Moll**_ is a Professor at the University of Tübingen endowed by the Carl Zeiss Foundation, at the Department of Computer Science and a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645.
</Acknowledgement>

<h2 className="left-align">BibTeX</h2>

```bibtex
@article{chen2024feat2gs,
  title={Feat2GS: Probing Visual Foundation Models with Gaussian Splatting},
  author={Chen, Yue and Chen, Xingyu and Chen, Anpei and Pons-Moll, Gerard and Xiu, Yuliang},
  journal={arXiv preprint arXiv:2412.09606},
  year={2024}
}
```