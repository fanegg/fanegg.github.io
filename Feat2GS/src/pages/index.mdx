---
layout: ../layouts/Layout.astro
title: "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting"
subtitle: "Probing Visual Foundation Models with Gaussian Splatting"
description: Project Page for Feat2GS
favicon: cat.svg
thumbnail: screenshot.jpg
---
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import QuoteSection from "../components/QuoteSection.astro";
import NoteSection from "../components/NoteSection.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import RadarChart from '../components/RadarChart.astro';
import Acknowledgement from "../components/Acknowledgement.astro";

import CompareVideos from "../components/CompareVideos.astro";
import MergeVideo from '../components/MergeVideo.astro';

import pipe from "../assets/pipe.mp4";
import motivation from "../assets/motivation.png";

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

import { Metric3dData } from "../data/Metric3d";
import { GeometryData } from "../data/Geometry.js";
import { TextureData, TextureConfig } from "../data/Texture.js";
import { AllData } from "../data/All.js";
import { ApplicationData } from "../data/Application.js";

<Header
  title={frontmatter.title}
  subtitle={frontmatter.subtitle}
  authors={[
    {
      name: "Yue Chen",
      url: "https://fanegg.github.io/",
      notes: ["1"],
    },
    {
      name: "Xingyu Chen",
      url: "https://rover-xingyu.github.io/",
      notes: ["1"],
    },
    {
      name: "Anpei Chen",
      url: "https://apchenstu.github.io/",
      notes: ["1,3"],

    },
    {
      name: "Gerard Pons-Moll",
      url: "https://virtualhumans.mpi-inf.mpg.de/",
      notes: ["3,4"],
    },
    {
      name: "Yuliang Xiu",
      url: "https://xiuyuliang.cn/",
      notes: ["1,2"],
    },
  ]}
  institutions={[
    { symbol: "1", name: "Westlake University" },
    { symbol: "2", name: "Max Planck Institute for Intelligent Systems" },
    { symbol: "3", name: "University of Tübingen, Tübingen AI Center" },
    { symbol: "4", name: "Max Planck Institute for Informatics, Saarland Informatics Campus" },
  ]} 
  links={[
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2412.09606",
      icon: "academicons:arxiv",
    },
    {
      name: "Code",
      url: "https://github.com/fanegg/Feat2GS",
      icon: "mdi:github",
    },
    {
      name: "Demo",
      url: "https://huggingface.co/spaces/endless-ai/Feat2GS",
      icon: "simple-icons:huggingface",
    },
    {
      name: "Gallery",
      url: "https://fanegg.github.io/feat2gs_gallery.html",
      icon: "fa-solid:images",
    },
    ]}
  />

<Image source={motivation} altText="Motivation" className="w-full max-w-[800px] mx-auto px-4 sm:px-0 overflow-x-hidden" />

<div className="text-center max-w-3xl mx-auto">
  We present **Feat2GS**, a unified framework to probe "texture and geometry awareness" of visual foundation models. Novel view synthesis serves as an effective proxy for 3D evaluation.
</div>

<HighlightedSection>

 ## How it works   
  <div style={{ textAlign: 'justify' }}>
  Casually captured photos are input into visual foundation models (VFMs) to extract features and into a stereo reconstructor to obtain relative poses. Pixel-wise features are transformed into 3D Gaussians (3DGS) using a lightweight readout layer trained with photometric loss. 3DGS parameters, grouped into **G**eometry and **T**exture, enable separate analysis of geometry/texture awareness in VFMs, evaluated by novel view synthesis (NVS) quality on diverse, unposed open-world images. We conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve SOTA across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for NVS.
  </div>
  <Video source={pipe} controls={false} />


</HighlightedSection>

## Video
  <YouTubeVideo videoId="4fT5lzcAJqo" />

## <a href="#chart" id="chart" className="no-underline hover:no-underline text-inherit outline-none cursor-default">Average for Novel View Synthesis across six datasets</a>
<RadarChart className="my-8" />

## <a href="#dtu" id="dtu" className="no-underline hover:no-underline text-inherit outline-none cursor-default">Novel View Synthesis _aligns well with_ Pointcloud Error Map</a>
<CompareVideos videos={Metric3dData} className="w-full sm:w-2/3 mx-auto" />

## Geometry Probing
<CompareVideos videos={GeometryData} />

## Texture Probing
<CompareVideos videos={TextureData} defaultLabels={TextureConfig.defaultLabels} />

## All=Geometry+Texture Probing
<CompareVideos videos={AllData} className="w-full sm:w-2/3 mx-auto" />

## Application
<MergeVideo videos={ApplicationData} />

<h2 className="left-align">Acknowledgement</h2>
<Acknowledgement>
  This work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 409792180 (EmmyNoether Programme, project: Real Virtual Humans), and German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. We thank _**Yuxuan Xue**_, _**Vladimir Guzov**_, _**Garvita Tiwari**_ for their valuable feedback, and the members of _**Endless AI Lab**_ and _**Real Virtual Humans**_ for their help and discussions. _**Yuliang Xiu**_ has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No.860768 (CLIPE project). _**Yue Chen**_ and _**Xingyu Chen**_ are supported by the Research Center for Industries of the Future (RCIF) at Westlake University, and the Westlake Education Foundation. _**Gerard Pons-Moll**_ is a Professor at the University of Tübingen endowed by the Carl Zeiss Foundation, at the Department of Computer Science and a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645.
</Acknowledgement>

<h2 className="left-align">BibTeX</h2>

```bibtex
@article{chen2024feat2gs,
  title={Feat2GS: Probing Visual Foundation Models with Gaussian Splatting},
  author={Chen, Yue and Chen, Xingyu and Chen, Anpei and Pons-Moll, Gerard and Xiu, Yuliang},
  journal={arXiv preprint arXiv:2412.09606},
  year={2024}
}
```