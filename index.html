<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="img/fanegg.gif">
  <title>Yue Chen - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Yue Chen (陈悦)</name>
        </p>
        <p>
          I obtained my Master's degree from <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a> in 2022, 
          following my undergraduate studies at <a href="https://en.xmu.edu.cn/">Xiamen University</a>.
        </p>  
        <p>
          From autumn 2021 to summer 2022, I had the fortunate opportunity to do a research internship at <a href="https://www.tencent.com/en-us/">Tencent</a>, 
          focusing on virtual humans.
        </p>
        <p>
          Currently, I'm on a gap year, savoring a wonderful journey in Yunnan, China. 
          The exciting news is that I'm about to join the <a href="https://virtualhumans.mpi-inf.mpg.de/">Real Virtual Humans Group</a> as a PhD student, 
          aiming to create realistic and flexible virtual humans to open the door to new generation virtual try-on and allow applications such as virtual-human based teachers.
        </p>  
        <p>
          I'm looking forward to starting my next research adventure in Tübingen!
        <p align=center>
          <a href="mailto:faneggchen@gmail.com">Email</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=M2hq1_UAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/fanegg">GitHub</a> &nbsp|&nbsp
          <a href="https://twitter.com/faneggchen">Twitter</a>
        </p>
        </td>
        <td width="33%">
          <a href="img/yue.jpg"><img src="img/fanegg.jpg" width="250" alt="headshot">
        </td>
      </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
              <li><strong>02/2023</strong> 2 papers (<a href="https://fanegg.github.io/UV-Volumes/">UV Volumes</a>, <a href="https://rover-xingyu.github.io/L2G-NeRF/">L2G-NeRF</a>) accepted to <strong>CVPR 2023</strong>! </li>
              <li><strong>03/2022</strong> 1 paper (<a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a>) accepted to <strong>CVPR 2022</strong>! </li>          
            </div></div>
            </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
            <!-- Representative papers are <span class="highlight">highlighted</span>. -->
            <!-- * denotes equal contribution co-authorship, † denotes corresponding author -->
            Equal Contribution * , Corresponding Author †
          </p>
        </td>
      </tr>
      </table>

      <table width="105%" align="center" border="0" cellspacing="0" cellpadding="20" >

        <tr onmouseout="uv_stop()" onmouseover="uv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='uv_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/uv_volume.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/retexture.png' width="160">
            </div>
            <script type="text/javascript">
              function uv_start() {
                document.getElementById('uv_image').style.opacity = "1";
              }
        
              function uv_stop() {
                document.getElementById('uv_image').style.opacity = "0";
              }
              uv_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://fanegg.github.io/UV-Volumes/">
              <papertitle>UV Volumes for Real-time Rendering of Editable Free-view Human Performance</papertitle>
            </a>
            <br>
            <strong>Yue Chen*</strong>,
            <a href="https://scholar.google.com/citations?user=h-3xd3EAAAAJ&hl=en">Xuan Wang*</a>,
            <a href="https://scholar.google.com/citations?user=gDHPrWEAAAAJ&hl=en">Xingyu Chen</a>,
            <a href="https://scholar.google.com/citations?user=2vFjhHMAAAAJ&hl=en">Qi Zhang</a>,
            <a href="https://scholar.google.com/citations?user=Dt0PcAYAAAAJ&hl=en">Xiaoyu Li</a>,
            <a href="https://scholar.google.com/citations?user=OemeiSIAAAAJ&hl=en">Yu Guo†</a>,
            <a href="https://scholar.google.com/citations?user=Bt4uDWMAAAAJ&hl=en">Jue Wang</a>,
            <a href="https://scholar.google.com/citations?user=uU2JTpUAAAAJ&hl=en">Fei Wang</a>
            <br>
            <i>CVPR 2023</i> 
            <br>
            <a href="https://fanegg.github.io/UV-Volumes/">project page</a> /
            <a href="https://arxiv.org/pdf/2203.14402.pdf">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf">paper</a> /
            <a href="https://github.com/fanegg/UV-Volumes">code /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_UV_Volumes_for_CVPR_2023_supplemental.pdf">supplementary</a> /
            <a href="https://youtu.be/JftQnXLMmPc">video</a></a>
            <p></p>
            <p> A deformable virtual human formulation, decomposing a dynamic virtual human into UV Volumes and an appearance texture, 
              which enables real-time rendering and texture editing with future applications in immersive VR/AR, video games and virtual teachers.</p>
          </td>
        </tr> 

    
        <tr onmouseout="l2g_stop()" onmouseover="l2g_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='l2g_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/l2g.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/l2g.jpg' width="160">
            </div>
            <script type="text/javascript">
              function l2g_start() {
                document.getElementById('l2g_image').style.opacity = "1";
              }

              function l2g_stop() {
                document.getElementById('l2g_image').style.opacity = "0";
              }
              l2g_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">
              <papertitle>L2G-NeRF: Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</papertitle>
            </a>
            <br>
            <strong>Yue Chen*</strong>,
            <a href="https://scholar.google.com/citations?user=gDHPrWEAAAAJ&hl=en">Xingyu Chen*</a>,
            <a href="https://scholar.google.com/citations?user=h-3xd3EAAAAJ&hl=en">Xuan Wang†</a>,
            <a href="https://scholar.google.com/citations?user=2vFjhHMAAAAJ&hl=en">Qi Zhang</a>,
            <a href="https://scholar.google.com/citations?user=OemeiSIAAAAJ&hl=en">Yu Guo†</a>,
            <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
            <a href="https://scholar.google.com/citations?user=uU2JTpUAAAAJ&hl=en">Fei Wang</a>
            <br >
            <i>CVPR 2023</i> 
            <br>
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">project page</a> /
            <a href="https://arxiv.org/pdf/2211.11505.pdf">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.pdf">paper</a> /
            <a href="https://github.com/rover-xingyu/L2G-NeRF">code</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_Local-to-Global_Registration_for_CVPR_2023_supplemental.pdf">supplementary</a> /
            <a href="https://youtu.be/y8XP9Umt6Mw">video</a>
            <p></p>
            <p>A local-to-global strategy for bundle-adjusting neural radiance fields, 
              combining the global and local alignment via parameter solver, 
              which enables democratizing graphic asset generation for next-generation social media.</p>
              
          </td>
        </tr>   
    
        <tr onmouseout="hanerf_stop()" onmouseover="hanerf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='hanerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/video-teaser_crop.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/teaser.png' width="160">
            </div>
            <script type="text/javascript">
              function hanerf_start() {
                document.getElementById('hanerf_image').style.opacity = "1";
              }

              function hanerf_stop() {
                document.getElementById('hanerf_image').style.opacity = "0";
              }
              hanerf_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">
              <papertitle>Ha-NeRF&#x1F606: Hallucinated Neural Radiance Fields in the Wild</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=gDHPrWEAAAAJ&hl=en">Xingyu Chen</a>,
            <a href="https://scholar.google.com/citations?user=2vFjhHMAAAAJ&hl=en">Qi Zhang†</a>,
            <a href="https://scholar.google.com/citations?user=Dt0PcAYAAAAJ&hl=en">Xiaoyu Li</a>,
            <strong>Yue Chen</strong>,
            <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
            <a href="https://scholar.google.com/citations?user=h-3xd3EAAAAJ&hl=en">Xuan Wang</a>,
            <a href="https://scholar.google.com/citations?user=Bt4uDWMAAAAJ&hl=en">Jue Wang</a>
            <br>
            <i>CVPR 2022</i> 
            <br>
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">project page</a> /
            <a href="https://arxiv.org/pdf/2111.15246.pdf">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Hallucinated_Neural_Radiance_Fields_in_the_Wild_CVPR_2022_paper.pdf">paper</a> /
            <a href="https://github.com/rover-xingyu/Ha-NeRF">code</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_CVPR_2022_supp.pdf">supplementary</a> /
            <a href="https://www.youtube.com/watch?v=Qrz0gfcTdQs">video</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_poster.pdf">poster</a>
            
            <p></p>
            <p>A framework to recover hallucinate photorealistic landscapes, 
              consistently rendering free-occlusion views under different appearance hallucinations, 
              which can be applied in virtual travel experiences and film special effects post-production.</p>
          </td>
        </tr> 

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
