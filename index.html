<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    .intro-text {
        font-size: 16px;
      }

    .intro-text a {
      font-size: 16px;
    }

    @media screen and (max-width: 768px) {
      .intro-text .intro-text a {
        font-size: 15px;
      }
    }
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }

    .magic-tabs {
      width: 100%;
      max-width: 700px; 
      min-height: 400px; 
      height: auto;
      margin: 20px auto 0 auto;
      background-color: #f4f4f5;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      display: flex;
      flex-direction: column;
      justify-content: space-between; 
      }

    .tab-content {
      position: relative;
      width: 400px;
      height: 400px;
      margin: 0 auto;
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
      border-radius: 8px;
      overflow: hidden;
    }

    .tab-pane {
      width: 100%;
      height: 100%;
      position: absolute;
      top: 0;
      left: 0;
      display: none;
    }

    .tab-pane.active {
    display: block;
    }

    .tab-pane video {
    width: 100%;
    height: 100%;
    object-fit: contain;
    }

    .tab-buttons {
    display: flex;
    gap: 0px;
    margin-top: 20px;
    flex-wrap: wrap;
    justify-content: center;
    min-height: 60px;
    }

    .tab-button {
    width: 70px;
    height: 70px;
    object-fit: cover;
    cursor: pointer;
    border: 2px solid transparent;
    transition: all 0.3s ease;
    opacity: 0.7;
    border-radius: 4px;
    }

    .tab-button:hover {
    opacity: 1;
    }

    .tab-button.active {
    border-color: #1772d0;
    opacity: 1;
    }

    .viewer-title {
      text-align: center;
      font-size: 24px;
      font-weight: 400;
      margin-top: 0px;
      margin-bottom: 0px;
      font-weight: normal;
      font-family: 'Roboto', sans-serif;
    }

    .viewer-note {
      text-align: center;
      font-size: 15px;
      margin-bottom: 20px;
      color: #333;
      padding: 0 10px;
      max-width: 700px;
      margin-left: auto;
      margin-right: auto;
    }
    .viewer-note a {
      font-size: 15px;
    }

  </style>
  <link rel="icon" type="image/svg+xml" href="img/cat.svg">
  <title>Yue Chen - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Yue Chen (陈悦)</name>
        </p>
        <p class="intro-text">I am currently a PhD student at <a href="https://en.westlake.edu.cn/">Westlake University</a>, co-supervised by <a href="https://xiuyuliang.cn/">Yuliang Xiu</a> and <a href="https://virtualhumans.mpi-inf.mpg.de/">Gerard Pons-Moll</a>.
        </p>
        <p class="intro-text">I am interested in modeling 3D/4D worlds from casually-captured real-world images and videos.
          I have previously done research internships at <a href="https://ailab.tencent.com/ailab/en/index/">Tencent AI Lab</a>, working with <a href="https://xuanwangvc.github.io/">Xuan Wang</a>.
          I completed my M.S. at <a href="http://en.xjtu.edu.cn">Xi'an Jiaotong University</a> and my B.S. at <a href="https://en.xmu.edu.cn/">Xiamen University</a>.
        </p>
        <!-- <p>I am fortunate to work with <a href="https://apchenstu.github.io/">Anpei Chen</a> at Inception3D Lab 
          and previously interned at <a href="https://ailab.tencent.com/ailab/en/index/">Tencent AI Lab</a> with <a href="https://xuanwangvc.github.io/">Xuan Wang</a>. 
          I got my M.S. from <a href="http://en.xjtu.edu.cn">Xi'an Jiaotong University</a> and my B.S. from <a href="https://en.xmu.edu.cn/">Xiamen University</a>.
        </p> -->
        <p class="intro-text">As a Muggle dreaming of the Wizarding World, I feel so fortunate to create magic through what I am exploring on this jorney.
        <p align=center>
          <a href="mailto:faneggchen@gmail.com">Email</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=M2hq1_UAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/fanegg">GitHub</a> &nbsp|&nbsp
          <a href="https://twitter.com/faneggchen">Twitter</a> 
        </p>
        </td>
        <td width="33%">
          <a href="img/yue.jpg"><img src="img/fanegg.jpg" width="100%" alt="headshot" style="border-radius: 50%; aspect-ratio: 1/1; object-fit: cover;"></a>
        </td>
      </tr>
      </table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
        </td>	
        <td width="10%" valign="middle">
          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a></td>
        </td>
        <td width="10%" valign="middle">
          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="50"></a></td>
        </td>	
      </tr>
      </table> -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!-- <tr>
        <td>
          <heading>News</heading>
            <ul> -->
<!--               <li><strong>03/2023</strong> <strong><span style="color:#ff0000;">
                I am actively looking for a Ph.D. position. 
                If possible, please <a href="mailto:fanegg@stu.xjtu.edu.cn"><strong>email me</strong></a> for further communication!</span></strong>
              </li> -->
              <!-- <li><strong>02/2023</strong> 2 papers (<a href="https://rover-xingyu.github.io/L2G-NeRF/">L2G-NeRF</a>, <a href="https://fanegg.github.io/UV-Volumes/">UV Volumes</a>) accepted to <strong>CVPR 2023</strong>! </li>
              <li><strong>01/2023</strong> 1 paper (<a href="https://arxiv.org/pdf/2211.11439.pdf">PROCA</a>) accepted to <strong>ICRA 2023</strong>! </li>
              <li><strong>03/2022</strong> 1 paper (<a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a>) accepted to <strong>CVPR 2022</strong>! </li> -->
              <!-- <li><strong>02/2022</strong> Invited to give a talk on large-scale scene reconstruction with NeRF at <a href="https://www.computationalimaging.org/">Stanford University</a> (<a href="files/talk_stanford.pdf">Slides</a>). </li>
              <li><strong>09/2022</strong> Our paper <a href="https://niujinshuchong.github.io/monosdf/"><strong>MonoSDF</strong></a> is accepted to <strong>NeurIPS 2022</strong>. </li>
              <li><strong>09/2022</strong> Invited to give a talk on neural rendering at <a href="https://research.adobe.com/">Adobe Research</a> (<a href="files/talk_adobe.pdf">Slides</a>). </li>
              <li><strong>06/2022</strong> This summer I will be a research intern at <a href="https://research.google">Google Research</a>. </li>
              <li><strong>06/2022</strong> <strong><span style="color:#ff0000;">1st place winner</span></strong> in partial object recovery and 2nd place overall of <a href="https://cvi2.uni.lu/sharp2022/">SHARP Challenge</a>! Congratulations on my students Lei, Zhizheng, Weining, and Liudi from 3D Vision course project at ETH Zurich!
              <li><strong>05/2022</strong> Selected as an <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at CVPR 2022. </li>
              <li><strong>03/2022</strong> Our paper <a href="https://pengsongyou.github.io/nice-slam"><strong>NICE-SLAM</strong></a> is accepted to <strong>CVPR 2022</strong>! </li>
              <li><strong>02/2022</strong> Invited to talk about <a href="https://pengsongyou.github.io/sap">Shape As Points</a> at <a href="https://twitter.com/talking_papers">Talking Papers Podcast</a>. Great chat with <a href="https://www.itzikbs.com/">Yizhak Ben-Shabat</a>!</li>
              <li><strong>12/2021</strong> Gave a talk again this year at <a href="http://games-cn.org/games-webinar-20211230-214/">GAMES Seminar Series</a> on <a href="https://pengsongyou.github.io/sap">Shape As Points</a>.</li>
              <li><strong>09/2021</strong> Our <a href="https://pengsongyou.github.io/sap">Shape As Points</a> is accepted to NeurIPS 2021 as <span style="color:#ff0000;"><strong>oral presentation</strong></span> <strong>(top 0.6%)</strong>!</li> -->
              <!-- <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;"> -->
              
            <!-- </div></div>
            </ul>
        </td>
      </tr> -->
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
            <!-- Representative papers are <span class="highlight">highlighted</span>. -->
            Equal Contribution * , Corresponding Author †, Project Lead ⚑
          </p>
        </td>
      </tr>
      </table>

      <table width="105%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <tr onmouseout="feat2gs_stop()" onmouseover="feat2gs_start()" style="height:100%">
          <td style="padding:10px 20px;width:25%;vertical-align:middle;height:100%">
            <div class="one" style="display:flex;align-items:center;height:100%">
              <div class="two" id='feat2gs_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/feat2gs.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/feat2gs.jpg' width="160">
            </div>
            <script type="text/javascript">
              function feat2gs_start() {
                document.getElementById('feat2gs_image').style.opacity = "1";
              }

              function feat2gs_stop() {
                document.getElementById('feat2gs_image').style.opacity = "0";
              }
              feat2gs_stop()
            </script>
          </td>
          <td style="padding:10px 20px 10px 10px;width:75%;vertical-align:middle">
            <a href="https://fanegg.github.io/Feat2GS/">
              <papertitle>Feat2GS: Probing Visual Foundation Models with Gaussian Splatting</papertitle>
            </a>
            <br>
            <strong>Yue Chen</strong>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
            <a href="https://apchenstu.github.io/">Anpei Chen</a>,
            <a href="https://virtualhumans.mpi-inf.mpg.de/">Gerard Pons-Moll</a>,
            <a href="https://xiuyuliang.cn/">Yuliang Xiu</a>
            <br >
            arXiv, 2024
            <br>
            <a href="https://fanegg.github.io/Feat2GS/">project page</a> /
            <a href="https://arxiv.org/abs/2412.09606">arXiv</a> /
            <a href="https://youtu.be/4fT5lzcAJqo">video</a>
            <p></p>
            <p>A unified framework to probe "<i>texture and geometry awareness</i>" of visual foundation models. Novel view synthesis serves as an effective proxy for 3D evaluation.</p>
              
          </td>
        </tr> 

      <table width="105%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <tr onmouseout="l2g_stop()" onmouseover="l2g_start()">
          <td style="padding:10px 20px;width:25%;vertical-align:middle;height:100%">
            <div class="one" style="display:flex;align-items:center;height:100%">
              <div class="two" id='l2g_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/l2g.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/l2g.jpg' width="160">
            </div>
            <script type="text/javascript">
              function l2g_start() {
                document.getElementById('l2g_image').style.opacity = "1";
              }

              function l2g_stop() {
                document.getElementById('l2g_image').style.opacity = "0";
              }
              l2g_stop()
            </script>
          </td>
          <td style="padding:10px 20px 10px 10px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">
              <papertitle>L2G-NeRF: Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</papertitle>
            </a>
            <br>
            <strong>Yue Chen*</strong>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen*⚑</a>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang†</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
            <a href="https://yuguo-xjtu.github.io/">Yu Guo†</a>,
            <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
            <a href="https://scholar.google.com/citations?user=uU2JTpUAAAAJ&hl=en">Fei Wang</a>
            <br >
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
            <br>
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">project page</a> /
            <a href="https://arxiv.org/abs/2211.11505">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.pdf">paper</a> /
            <a href="https://github.com/rover-xingyu/L2G-NeRF">code</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_Local-to-Global_Registration_for_CVPR_2023_supplemental.pdf">supplementary</a> /
            <a href="https://youtu.be/y8XP9Umt6Mw">video</a> /
            <a href="https://rover-xingyu.github.io/L2G-NeRF/files/l2gnerf_poster.pdf">poster</a>
            <p></p>
            <p>A Local-to-Global robust registration strategy for bundle-adjusting NeRF, 
              combining the global and local alignment via differentiable parameter estimation.</p>
              
          </td>
        </tr>   

        <tr onmouseout="uv_stop()" onmouseover="uv_start()">
          <td style="padding:10px 20px;width:25%;vertical-align:middle;height:100%">
            <div class="one" style="display:flex;align-items:center;height:100%">
              <div class="two" id='uv_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/uvvolume.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/uvvolume.jpg' width="160">
            </div>
            <script type="text/javascript">
              function uv_start() {
                document.getElementById('uv_image').style.opacity = "1";
              }
        
              function uv_stop() {
                document.getElementById('uv_image').style.opacity = "0";
              }
              uv_stop()
            </script>
          </td>
          <td style="padding:10px 20px 10px 10px;width:75%;vertical-align:middle">
            <a href="https://fanegg.github.io/UV-Volumes/">
              <papertitle>UV Volumes for Real-time Rendering of Editable Free-view Human Performance</papertitle>
            </a>
            <br>
            <strong>Yue Chen*</strong>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang*</a>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
            <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
            <a href="https://yuguo-xjtu.github.io/">Yu Guo†</a>,
            <a href="https://juewang725.github.io/">Jue Wang</a>,
            <a href="https://scholar.google.com/citations?user=uU2JTpUAAAAJ&hl=en">Fei Wang</a>
            <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
            <br>
            <a href="https://fanegg.github.io/UV-Volumes/">project page</a> /
            <a href="https://arxiv.org/abs/2203.14402">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf">paper</a> /
            <a href="https://github.com/fanegg/UV-Volumes">code /
            <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_UV_Volumes_for_CVPR_2023_supplemental.pdf">supplementary</a> /
            <a href="https://youtu.be/JftQnXLMmPc">video</a></a> /
            <a href="UV-Volumes/files/uvvolumes_poster.pdf">poster</a>
            <p></p>
            <p> An editable, real-time rendering, and deformable human NeRF, decomposing a dynamic human into 3D UV Volumes and a 2D appearance texture.</p>
          </td>
        </tr> 
        
      
        <tr onmouseout="PROCA_stop()" onmouseover="PROCA_start()">
          <td style="padding:10px 20px;width:25%;vertical-align:middle;height:100%">
            <div class="one" style="display:flex;align-items:center;height:100%">
              <div class="two" id='PROCA_image'>
                <img src='img/proca2.jpg' width="160">
              </div>
              <img src='img/proca1.jpg' width="160">
            </div>
            <script type="text/javascript">
              function PROCA_start() {
                document.getElementById('PROCA_image').style.opacity = "1";
              }
        
              function PROCA_stop() {
                document.getElementById('PROCA_image').style.opacity = "0";
              }
              PROCA_stop()
            </script>
          </td>
          <td style="padding:10px 20px 10px 10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2211.11439">
              <papertitle>PROCA: Place Recognition under Occlusion and Changing Appearance via Disentangled Representations</papertitle>
            </a>
            <br>
            <strong>Yue Chen</strong>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen†⚑</a>,
            <a href="https://yicen-research.webador.com/">Yicen Li</a>
            <br>
            IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023
            <br>
            <a href="https://arxiv.org/abs/2211.11439">arXiv</a> /
            <a href="https://github.com/rover-xingyu/PROCA">code</a> /
            <a href="https://www.youtube.com/watch?v=W_tol4aHIQk">video</a> /
            <a href="https://fanegg.github.io/PROCA/PROCA_poster.pdf">poster</a>
            <p></p>
            <p>
              An unsupervised approach to disentangle a scene into place, appearance, and occlusion features, handling place recognition under occlusion and varying appearance.
            </p>
          </td>
        </tr>       
    
        <tr onmouseout="hanerf_stop()" onmouseover="hanerf_start()">
          <td style="padding:10px 20px;width:25%;vertical-align:middle;height:100%">
            <div class="one" style="display:flex;align-items:center;height:100%">
              <div class="two" id='hanerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/hanerf.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/hanerf.jpg' width="160">
            </div>
            <script type="text/javascript">
              function hanerf_start() {
                document.getElementById('hanerf_image').style.opacity = "1";
              }

              function hanerf_stop() {
                document.getElementById('hanerf_image').style.opacity = "0";
              }
              hanerf_stop()
            </script>
          </td>
          <td style="padding:10px 20px 10px 10px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">
              <papertitle>Ha-NeRF&#x1F606: Hallucinated Neural Radiance Fields in the Wild</papertitle>
            </a>
            <br>
            <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang†</a>,
            <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
            <strong>Yue Chen</strong>,
            <a href="https://fanegg.github.io/">Ying Feng</a>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
            <a href="https://juewang725.github.io/">Jue Wang</a>
            <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022
            <br>
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">project page</a> /
            <a href="https://arxiv.org/abs/2111.15246">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Hallucinated_Neural_Radiance_Fields_in_the_Wild_CVPR_2022_paper.pdf">paper</a> /
            <a href="https://github.com/rover-xingyu/Ha-NeRF">code</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_CVPR_2022_supp.pdf">supplementary</a> /
            <a href="https://www.youtube.com/watch?v=Qrz0gfcTdQs">video</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_poster.pdf">poster</a>
            
            <p></p>
            <p>A method to recover 3D scenes from tourism photos with various styles and occlusions,
               consistently rendering occlusion-free views under different appearance hallucinations.</p>
          </td>
        </tr> 
  
      <!-- <table width="105%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <br>
        <tr>
          <td>
            <heading>Projects</heading>
          </td>
        </tr> -->
        <!-- fiber -->
        <!-- <tr onmouseout="fiber_stop()" onmouseover="fiber_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='fiber_image'>
                <img src='img/exp.png' width="160">
              </div>
              <img src='img/rec.png' width="160">
            </div>
            <script type="text/javascript">
              function fiber_start() {
                document.getElementById('fiber_image').style.opacity = "1";
              }

              function fiber_stop() {
                document.getElementById('fiber_image').style.opacity = "0";
              }
              fiber_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://fanegg.github.io/">
              <papertitle>Deformation Field Estimation Using Fiber Bragg Grating Sensors</papertitle>
            </a>
            <br>
            <strong>Yue Chen</strong>,
            <a href="https://scholar.google.com.hk/citations?user=bfOeljkAAAAJ&hl=zh-CN&oi=sra">Hu Sun</a>
            <br>
            <em>Xiamen University Outstanding Undergraduate Thesis Award</em>, 2019
            <br>
            <p></p>
            <p>
              A study on deformation field estimation for the wing physical model using the Fiber Bragg Grating sensing network,
              which can be applied in aerospace structural health monitoring.
            </p>
          </td>
        </tr> 
      </table> -->

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <br>
        <tr>
          <td>
            <heading>Invited Talks</heading>
          </td>
        </tr>
        <tr onmouseout="HaNeRF_talk_stop()" onmouseover="HaNeRF_talk_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='HaNeRF_talk_image'><video  width=100% height=100% muted autoplay loop>
              <img src='img/HaNeRF_talk.png' width="160">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/HaNeRF_talk.png' width="160">
            </div>
            <script type="text/javascript">
              function HaNeRF_talk_start() {
                document.getElementById('HaNeRF_talk_image').style.opacity = "1";
              }
              function HaNeRF_talk_stop() {
                document.getElementById('HaNeRF_talk_image').style.opacity = "0";
              }
              HaNeRF_talk_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.shenlanxueyuan.com/open/course/155">
              <papertitle>光影幻象：神经辐射场中的时空流转</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>
            <br>
            <em>Shenlan College online education</em>, 2022
            <br>
            <p></p>
            <p>Introduction about Neural Radiance Fields (NeRF) for unconstrained photo collections.
            <br>Including <a href="https://www.matthewtancik.com/nerf">NeRF</a>, <a href="https://nerf-w.github.io/">NeRF in the Wild</a>, and <a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a> 
            </p>
          </td>
        </tr> 
      </table> -->

      <!-- Academic Services -->
      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <heading>Academic Services</heading>
              <ul>
                <li><strong>Conference Reviewer</strong>: CVPR, SIGGRAPH
              </ul>
          </td>
        </tr>
        </table> -->
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
           <td>
          <div class="magic-tabs">
            <p class="viewer-title">🪄Created Magic</p>
            <p class="viewer-note">Making cool stuff is my jam — like rebuilding Hogwarts from <i>Harry Potter</i> movies or recreating lovely moments in 3D from my daily photos.
              &#x1F5B1;&#xFE0F;<i>Click on the images below to render 3D, powered by <a href="https://fanegg.github.io/Feat2GS/">Feat2GS</a>.</i>
            </p>
           <div class="tab-content">
             <div class="tab-pane active" id="video1">
               <video width="100%" loop muted>
                 <source src="img/hogwarts.mp4" type="video/mp4">
               </video>
             </div>
             <div class="tab-pane" id="video2">
               <video width="100%" loop muted>
                 <source src="img/hogwarts2.mp4" type="video/mp4">
               </video>
             </div>
             <div class="tab-pane" id="video3">
               <video width="100%" loop muted>
                 <source src="img/paper.mp4" type="video/mp4">
               </video>
             </div>
             <div class="tab-pane" id="video4">
              <video width="100%" loop muted>
                <source src="img/paper2.mp4" type="video/mp4">
              </video>
            </div>
             <div class="tab-pane" id="video5">
              <video width="100%" autoplay loop muted>
                <source src="img/coffee.mp4" type="video/mp4">
              </video>
            </div>
            <div class="tab-pane" id="video6">
              <video width="100%" loop muted>
                <source src="img/home.mp4" type="video/mp4">
              </video>
            </div>
            <div class="tab-pane" id="video7">
              <video width="100%" loop muted>
                <source src="img/yu.mp4" type="video/mp4">
              </video>
            </div>
            <div class="tab-pane" id="video8">
              <video width="100%" loop muted>
                <source src="img/cy2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="tab-pane" id="video9">
              <video width="100%" loop muted>
                <source src="img/cy.mp4" type="video/mp4">
              </video>
            </div>
           </div>
          <div class="tab-buttons">
            <img src="img/hogwarts.jpg" class="tab-button active" data-tab="video1">
            <img src="img/hogwarts2.jpg" class="tab-button" data-tab="video2">
            <img src="img/paper.jpg" class="tab-button" data-tab="video3">
            <img src="img/paper2.jpg" class="tab-button" data-tab="video4">
            <img src="img/coffee.jpg" class="tab-button" data-tab="video5">
            <img src="img/home.jpg" class="tab-button" data-tab="video6">
            <img src="img/yu.jpg" class="tab-button" data-tab="video7">
            <img src="img/cy2.jpg" class="tab-button" data-tab="video8">
            <img src="img/cy.jpg" class="tab-button" data-tab="video9">
          </div>
          </div>
        </td>
      </tr>
    </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
            Website source code from <a href="https://jonbarron.info/"><font size="2">Jon Barron</font></a>
          <br>
          Last updated: Dec 2024
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->

      <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function() {
          const tabButtons = document.querySelectorAll('.tab-button');
          const videos = document.querySelectorAll('.tab-pane video');
          
          function stopAllVideos() {
            videos.forEach(video => {
              video.pause();
            });
          }
          
          videos.forEach(video => {
            video.addEventListener('click', function() {
              if (video.paused) {
                stopAllVideos();
                video.play(); 
              } else {
                video.pause();
              }
            });
          });
          
          tabButtons.forEach(button => {
            button.addEventListener('click', function() {
              tabButtons.forEach(btn => btn.classList.remove('active'));
              document.querySelectorAll('.tab-pane').forEach(pane => pane.classList.remove('active'));
              
              stopAllVideos();
              
              this.classList.add('active');
              const tabId = this.getAttribute('data-tab');
              const activePane = document.getElementById(tabId);
              activePane.classList.add('active');
              
              const currentVideo = activePane.querySelector('video');
              if (currentVideo) {
                currentVideo.play();
              }
            });
          });

          stopAllVideos();
        });
      </script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
