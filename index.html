<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="img/cat.jpg">
  <title>Yue Chen - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Yue Chen (陈悦)</name>
        </p>
        <p>
          I obtained my Master’s degree from the <a href="http://www.aiar.xjtu.edu.cn/#">Institute of Artificial Intelligence and Robotics</a> 
          at <a href="http://en.xjtu.edu.cn">Xi’an Jiaotong University</a> in 2022, 
          supervised by <a href="https://scholar.google.com/citations?user=-JPp4nQAAAAJ&hl=zh-CN">Prof. Fei Wang</a> 
          and <a href="https://yuguo-xjtu.github.io/">Prof.Yu Guo</a>.
          I did my undergrad at <a href="https://en.xmu.edu.cn/">Xiamen University</a>.
        </p>  
        <p>
          During my master, I was a research intern in Visual Computing Center at <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI lab</a>, 
          working with <a href="https://juewang725.github.io/">Jue Wang</a> and <a href="https://xuanwangvc.github.io/">Xuan Wang</a>.
        </p>
        <!-- <p>
          My topics of interest include computer vision and computer graphics. 
          Currently, I am particularly interested in neural fields.
        </p> -->
        <p>
          I am a Muggle dreaming of the Wizarding World. 
          Luckily, I can create magic via <strong>neural fields</strong> and a closed loop formed by <strong>computer vision</strong> and <strong>computer graphics</strong> between 2D images and the 3D world!
        </p>
        <p align=center>
          <a href="mailto:fanegg@stu.xjtu.edu.cn">Email</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=M2hq1_UAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/fanegg">GitHub</a> 
        </p>
        </td>
        <td width="33%">
          <img src="img/fanegg.jpg" width="250" alt="headshot">
        </td>
      </tr>
      </table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="10%" valign="middle">
          <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
        </td>
        <td width="10%" valign="middle">
          <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
        </td>	
        <td width="10%" valign="middle">
          <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a></td>
        </td>
        <td width="10%" valign="middle">
          <a href="https://en.xjtu.edu.cn/"><img src="media/xjtu_logo.png" width="50"></a></td>
        </td>	
      </tr>
      </table> -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
<!--               <li><strong>03/2023</strong> <strong><span style="color:#ff0000;">
                I am actively looking for a Ph.D. position. 
                If possible, please <a href="mailto:fanegg@stu.xjtu.edu.cn"><strong>email me</strong></a> for further communication!</span></strong>
              </li> -->
              <li><strong>02/2023</strong> 2 papers (<a href="https://rover-xingyu.github.io/L2G-NeRF/">L2G-NeRF</a>, <a href="https://fanegg.github.io/UV-Volumes/">UV Volumes</a>) accepted to <strong>CVPR 2023</strong>! </li>
              <li><strong>01/2023</strong> 1 paper (<a href="https://arxiv.org/pdf/2211.11439.pdf">PROCA</a>) accepted to <strong>ICRA 2023</strong>! </li>
              <li><strong>03/2022</strong> 1 paper (<a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a>) accepted to <strong>CVPR 2022</strong>! </li>
              <!-- <li><strong>02/2022</strong> Invited to give a talk on large-scale scene reconstruction with NeRF at <a href="https://www.computationalimaging.org/">Stanford University</a> (<a href="files/talk_stanford.pdf">Slides</a>). </li>
              <li><strong>09/2022</strong> Our paper <a href="https://niujinshuchong.github.io/monosdf/"><strong>MonoSDF</strong></a> is accepted to <strong>NeurIPS 2022</strong>. </li>
              <li><strong>09/2022</strong> Invited to give a talk on neural rendering at <a href="https://research.adobe.com/">Adobe Research</a> (<a href="files/talk_adobe.pdf">Slides</a>). </li>
              <li><strong>06/2022</strong> This summer I will be a research intern at <a href="https://research.google">Google Research</a>. </li>
              <li><strong>06/2022</strong> <strong><span style="color:#ff0000;">1st place winner</span></strong> in partial object recovery and 2nd place overall of <a href="https://cvi2.uni.lu/sharp2022/">SHARP Challenge</a>! Congratulations on my students Lei, Zhizheng, Weining, and Liudi from 3D Vision course project at ETH Zurich!
              <li><strong>05/2022</strong> Selected as an <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">outstanding reviewer</a> at CVPR 2022. </li>
              <li><strong>03/2022</strong> Our paper <a href="https://pengsongyou.github.io/nice-slam"><strong>NICE-SLAM</strong></a> is accepted to <strong>CVPR 2022</strong>! </li>
              <li><strong>02/2022</strong> Invited to talk about <a href="https://pengsongyou.github.io/sap">Shape As Points</a> at <a href="https://twitter.com/talking_papers">Talking Papers Podcast</a>. Great chat with <a href="https://www.itzikbs.com/">Yizhak Ben-Shabat</a>!</li>
              <li><strong>12/2021</strong> Gave a talk again this year at <a href="http://games-cn.org/games-webinar-20211230-214/">GAMES Seminar Series</a> on <a href="https://pengsongyou.github.io/sap">Shape As Points</a>.</li>
              <li><strong>09/2021</strong> Our <a href="https://pengsongyou.github.io/sap">Shape As Points</a> is accepted to NeurIPS 2021 as <span style="color:#ff0000;"><strong>oral presentation</strong></span> <strong>(top 0.6%)</strong>!</li> -->
              <!-- <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
              <div id="old_news" style="display: none;"> -->
              
            </div></div>
            </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
            <!-- Representative papers are <span class="highlight">highlighted</span>. -->
            <!-- * denotes equal contribution co-authorship, † denotes corresponding author -->
            Equal Contribution * , Corresponding Author †
          </p>
        </td>
      </tr>
      </table>

      <table width="105%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <tr onmouseout="l2g_stop()" onmouseover="l2g_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='l2g_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/l2g.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/l2g.jpg' width="160">
            </div>
            <script type="text/javascript">
              function l2g_start() {
                document.getElementById('l2g_image').style.opacity = "1";
              }

              function l2g_stop() {
                document.getElementById('l2g_image').style.opacity = "0";
              }
              l2g_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">
              <papertitle>L2G-NeRF: Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</papertitle>
            </a>
            <br>
            <strong>Yue Chen*</strong>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen*</a>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang†</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
            <a href="https://yuguo-xjtu.github.io/">Yu Guo†</a>,
            <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
            <a href="https://scholar.google.com/citations?user=-JPp4nQAAAAJ&hl=zh-CN">Fei Wang</a>
            <br >
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
            <br>
            <a href="https://rover-xingyu.github.io/L2G-NeRF/">project page</a> /
            <a href="https://arxiv.org/pdf/2211.11505.pdf">arXiv</a> /
            <a href="https://github.com/rover-xingyu/L2G-NeRF">code</a> /
            <!-- <a href="https://fanegg.github.io/UV-Volumes/files/UV_Volumes_Supplementary_Material.pdf">supplementary</a> / -->
            <a href="https://youtu.be/y8XP9Umt6Mw">video</a> /
            <a href="https://rover-xingyu.github.io/L2G-NeRF/files/l2gnerf_poster.pdf">poster</a>
            <p></p>
            <p>A Local-to-Global robust registration strategy for bundle-adjusting Neural Radiance Fields, 
              combining the global and local alignment via differentiable parameter estimation.</p>
              
          </td>
        </tr>   

        <tr onmouseout="uv_stop()" onmouseover="uv_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='uv_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/uv_volume.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/retexture.png' width="160">
            </div>
            <script type="text/javascript">
              function uv_start() {
                document.getElementById('uv_image').style.opacity = "1";
              }
        
              function uv_stop() {
                document.getElementById('uv_image').style.opacity = "0";
              }
              uv_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://fanegg.github.io/UV-Volumes/">
              <papertitle>UV Volumes for Real-time Rendering of Editable Free-view Human Performance</papertitle>
            </a>
            <br>
            <strong>Yue Chen*</strong>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang*</a>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang</a>,
            <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
            <a href="https://yuguo-xjtu.github.io/">Yu Guo†</a>,
            <a href="https://juewang725.github.io/">Jue Wang</a>,
            <a href="https://scholar.google.com/citations?user=-JPp4nQAAAAJ&hl=zh-CN">Fei Wang</a>
            <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
            <br>
            <a href="https://fanegg.github.io/UV-Volumes/">project page</a> /
            <a href="https://arxiv.org/pdf/2203.14402.pdf">arXiv</a> /
            <a href="https://github.com/fanegg/UV-Volumes">code /
            <a href="UV-Volumes/files/UV_Volumes_Supplementary_Material.pdf">supplementary</a> /
            <a href="https://youtu.be/JftQnXLMmPc">video</a></a> /
            <a href="UV-Volumes/files/uvvolumes_poster.pdf">poster</a>
            <p></p>
            <p> The first deformable, real-time rendering, and editable NeRF, decomposing a dynamic human into 3D UV Volumes and a 2D appearance texture.</p>
          </td>
        </tr> 
        
      
        <tr onmouseout="PROCA_stop()" onmouseover="PROCA_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='PROCA_image'>
                <img src='img/proca2.jpg' width="160">
              </div>
              <img src='img/proca1.jpg' width="160">
            </div>
            <script type="text/javascript">
              function PROCA_start() {
                document.getElementById('PROCA_image').style.opacity = "1";
              }
        
              function PROCA_stop() {
                document.getElementById('PROCA_image').style.opacity = "0";
              }
              PROCA_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2211.11439.pdf">
              <papertitle>PROCA: Place Recognition under Occlusion and Changing Appearance via Disentangled Representations</papertitle>
            </a>
            <br>
            <strong>Yue Chen</strong>,
            <a href="https://rover-xingyu.github.io/">Xingyu Chen†</a>,
            <a href="https://github.com/YicenJoJo">Yicen Li</a>
            <br>
            IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023
            <br>
            <a href="https://arxiv.org/pdf/2211.11439.pdf">arXiv</a> /
            <a href="https://github.com/rover-xingyu/PROCA">code</a> /
            <a href="https://www.youtube.com/watch?v=W_tol4aHIQk">video</a> /
            <a href="https://fanegg.github.io/PROCA/PROCA_poster.pdf">poster</a>
            <p></p>
            <p>
              An unsupervised approach to disentangle a scene into place, appearance, and occlusion features, 
              where the place feature significantly increases the recall of place recognition under occlusion and changing appearance.
            </p>
          </td>
        </tr>       
    
        <tr onmouseout="hanerf_stop()" onmouseover="hanerf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='hanerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/video-teaser_crop.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/teaser.png' width="160">
            </div>
            <script type="text/javascript">
              function hanerf_start() {
                document.getElementById('hanerf_image').style.opacity = "1";
              }

              function hanerf_stop() {
                document.getElementById('hanerf_image').style.opacity = "0";
              }
              hanerf_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">
              <papertitle>Ha-NeRF&#x1F606: Hallucinated Neural Radiance Fields in the Wild</papertitle>
            </a>
            <br>
            <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
            <a href="https://qzhang-cv.github.io/">Qi Zhang†</a>,
            <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
            <strong>Yue Chen</strong>,
            <a href="https://fanegg.github.io/">Ying Feng</a>,
            <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
            <a href="https://juewang725.github.io/">Jue Wang</a>
            <br>
            IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022
            <br>
            <a href="https://rover-xingyu.github.io/Ha-NeRF/">project page</a> /
            <a href="https://arxiv.org/pdf/2111.15246.pdf">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Hallucinated_Neural_Radiance_Fields_in_the_Wild_CVPR_2022_paper.pdf">paper</a> /
            <a href="https://github.com/rover-xingyu/Ha-NeRF">code</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_CVPR_2022_supp.pdf">supplementary</a> /
            <a href="https://www.youtube.com/watch?v=Qrz0gfcTdQs">video</a> /
            <a href="https://rover-xingyu.github.io/Ha-NeRF/files/Ha_NeRF_poster.pdf">poster</a>
            
            <p></p>
            <p>A framework to recover hallucinated neural radiance fields from tourism images with variable appearance and occlusions,
               consistently rendering free-occlusion views under different appearance hallucinations.</p>
          </td>
        </tr> 
  
      <table width="105%" align="center" border="0" cellspacing="0" cellpadding="20" >
        <br>
        <tr>
          <td>
            <heading>Projects</heading>
          </td>
        </tr>
  <!-- fiber -->
        <tr onmouseout="fiber_stop()" onmouseover="fiber_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='fiber_image'>
                <img src='img/exp.png' width="160">
              </div>
              <img src='img/rec.png' width="160">
            </div>
            <script type="text/javascript">
              function fiber_start() {
                document.getElementById('fiber_image').style.opacity = "1";
              }

              function fiber_stop() {
                document.getElementById('fiber_image').style.opacity = "0";
              }
              fiber_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://fanegg.github.io/">
              <papertitle>Deformation Field Estimation Using Fiber Bragg Grating Sensors</papertitle>
            </a>
            <br>
            <strong>Yue Chen</strong>,
            <a href="https://scholar.google.com.hk/citations?user=bfOeljkAAAAJ&hl=zh-CN&oi=sra">Hu Sun</a>
            <br>
            <em>Xiamen University Outstanding Undergraduate Thesis Award</em>, 2019
            <br>
            <p></p>
            <p>
              A study on deformation field estimation for the wing physical model using the Fiber Bragg Grating sensing network,
              which can be applied in aerospace structural health monitoring.
              <!-- Deformation monitoring is vital for the flexible wing, morphing aircraft, and other aircraft. 
              Fiber Bragg Grating (FBG) sensing technology, as a new monitoring and sensing technology, has been widely used in aerospace structural health monitoring.
              This paper studies the structure deformation monitoring for the wing model using the FBG sensing network.  -->
            </p>
          </td>
        </tr> 
      </table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <br>
        <tr>
          <td>
            <heading>Invited Talks</heading>
          </td>
        </tr>
        <tr onmouseout="HaNeRF_talk_stop()" onmouseover="HaNeRF_talk_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='HaNeRF_talk_image'><video  width=100% height=100% muted autoplay loop>
              <img src='img/HaNeRF_talk.png' width="160">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/HaNeRF_talk.png' width="160">
            </div>
            <script type="text/javascript">
              function HaNeRF_talk_start() {
                document.getElementById('HaNeRF_talk_image').style.opacity = "1";
              }
              function HaNeRF_talk_stop() {
                document.getElementById('HaNeRF_talk_image').style.opacity = "0";
              }
              HaNeRF_talk_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.shenlanxueyuan.com/open/course/155">
              <papertitle>光影幻象：神经辐射场中的时空流转</papertitle>
            </a>
            <br>
            <strong>Xingyu Chen</strong>
            <br>
            <em>Shenlan College online education</em>, 2022
            <br>
            <p></p>
            <p>Introduction about Neural Radiance Fields (NeRF) for unconstrained photo collections.
            <br>Including <a href="https://www.matthewtancik.com/nerf">NeRF</a>, <a href="https://nerf-w.github.io/">NeRF in the Wild</a>, and <a href="https://rover-xingyu.github.io/Ha-NeRF/">Ha-NeRF</a> 
            </p>
          </td>
        </tr> 
      </table> -->

      <!-- Academic Services -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <heading>Academic Services</heading>
              <ul>
                <li><strong>Conference Reviewer</strong>: CVPR
              </ul>
          </td>
        </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
          <br>
          Last updated: Mar 2023
        </font>
        </p>
        </td>
      </tr>
      </table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-116734954-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116734954-1');
</script>
    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
